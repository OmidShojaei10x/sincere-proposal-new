#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ÛŒ Ø§ÛŒÙ†Ø³ØªØ§Ú¯Ø±Ø§Ù… Ø¨Ø§ Selenium
Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù†ØµØ¨: pip install selenium requests
"""

import os
import time
import json
import requests
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import re

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
INSTAGRAM_URL = "https://www.instagram.com/sincere.restaurant/"
DOWNLOAD_FOLDER = "/Users/omid/Downloads/Omid_Shojaei/Proposal/sincere/Sincere"
SCROLL_PAUSE_TIME = 2
MAX_SCROLLS = 10  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ú©Ø±ÙˆÙ„

def setup_driver():
    """ØªÙ†Ø¸ÛŒÙ… Chrome Driver"""
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # User Agent ÙˆØ§Ù‚Ø¹ÛŒ
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Chrome Driver: {e}")
        print("ğŸ’¡ Ù„Ø·ÙØ§Ù‹ ChromeDriver Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² Ø±ÙˆØ´ Ø¯Ø³ØªÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯")
        return None

def ensure_folder():
    """Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯"""
    Path(DOWNLOAD_FOLDER).mkdir(parents=True, exist_ok=True)
    print(f"âœ… Ù¾ÙˆØ´Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯: {DOWNLOAD_FOLDER}")

def scroll_page(driver, max_scrolls=MAX_SCROLLS):
    """Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ù¾Ø³Øªâ€ŒÙ‡Ø§"""
    print("ğŸ“œ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡...")
    last_height = driver.execute_script("return document.body.scrollHeight")
    scrolls = 0
    
    while scrolls < max_scrolls:
        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ†
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_PAUSE_TIME)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø±ØªÙØ§Ø¹ Ø¬Ø¯ÛŒØ¯
        new_height = driver.execute_script("return document.body.scrollHeight")
        
        if new_height == last_height:
            break
        
        last_height = new_height
        scrolls += 1
        print(f"  Ø§Ø³Ú©Ø±ÙˆÙ„ {scrolls}/{max_scrolls}...")
    
    print(f"âœ… Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯ ({scrolls} Ø¨Ø§Ø±)")

def extract_media_urls(driver):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ URLÙ‡Ø§ÛŒ ØªØµØ§ÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§"""
    print("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ URLÙ‡Ø§...")
    
    urls = {
        'images': [],
        'videos': []
    }
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ±
    try:
        images = driver.find_elements(By.CSS_SELECTOR, 'img[src*="scontent"], img[src*="cdninstagram"]')
        for img in images:
            src = img.get_attribute('src')
            if src and 'scontent' in src and src not in urls['images']:
                # Ø­Ø°Ù query parameters Ø¨Ø±Ø§ÛŒ URL Ø§ØµÙ„ÛŒ
                clean_url = src.split('?')[0]
                if clean_url not in urls['images']:
                    urls['images'].append(clean_url)
        
        # Ø¨Ø±Ø±Ø³ÛŒ srcset
        images_with_srcset = driver.find_elements(By.CSS_SELECTOR, 'img[srcset]')
        for img in images_with_srcset:
            srcset = img.get_attribute('srcset')
            if srcset:
                for src in srcset.split(','):
                    url = src.strip().split(' ')[0]
                    if 'scontent' in url and url not in urls['images']:
                        clean_url = url.split('?')[0]
                        if clean_url not in urls['images']:
                            urls['images'].append(clean_url)
    except Exception as e:
        print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ±: {e}")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§
    try:
        videos = driver.find_elements(By.CSS_SELECTOR, 'video source, video[src]')
        for video in videos:
            src = video.get_attribute('src')
            if src and src not in urls['videos']:
                clean_url = src.split('?')[0]
                if clean_url not in urls['videos']:
                    urls['videos'].append(clean_url)
    except Exception as e:
        print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§: {e}")
    
    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ØªØµØ§ÙˆÛŒØ± Ú©ÙˆÚ†Ú©
    urls['images'] = [url for url in urls['images'] 
                     if not any(x in url.lower() for x in ['profile_pic', 'avatar', 'icon'])]
    
    print(f"âœ… Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(urls['images'])} ØªØµÙˆÛŒØ±ØŒ {len(urls['videos'])} ÙˆÛŒØ¯ÛŒÙˆ")
    return urls

def download_file(url, folder, filename=None, retry=3):
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒÚ© ÙØ§ÛŒÙ„"""
    try:
        if not filename:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø² URL
            parsed_url = url.split('?')[0]
            url_filename = os.path.basename(parsed_url)
            
            if not url_filename or '.' not in url_filename:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² hash
                import hashlib
                url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
                ext = 'webp' if 'webp' in url else ('mp4' if 'mp4' in url else 'jpg')
                filename = f"sincere-media-{url_hash}.{ext}"
            else:
                filename = re.sub(r'[<>:"/\\|?*]', '_', url_filename)
        
        filepath = os.path.join(folder, filename)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
        if os.path.exists(filepath):
            print(f"â­ï¸  Ù…ÙˆØ¬ÙˆØ¯: {filename}")
            return True
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Referer': 'https://www.instagram.com/',
        }
        
        response = requests.get(url, headers=headers, stream=True, timeout=30)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        file_size = os.path.getsize(filepath) / 1024
        print(f"âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯: {filename} ({file_size:.1f} KB)")
        return True
        
    except Exception as e:
        if retry > 0:
            time.sleep(2)
            return download_file(url, folder, filename, retry-1)
        else:
            print(f"âŒ Ø®Ø·Ø§: {url[:50]}... - {e}")
            return False

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ÛŒÙ†Ø³ØªØ§Ú¯Ø±Ø§Ù… Ø³ÛŒÙ†Ø³ÛŒØ±\n")
    
    ensure_folder()
    
    driver = setup_driver()
    if not driver:
        print("\nğŸ’¡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø±ÙˆØ´ Ø¯Ø³ØªÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:")
        print("   1. ÙØ§ÛŒÙ„ extract_urls.js Ø±Ø§ Ø¯Ø± Console Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")
        print("   2. Ø³Ù¾Ø³ download_instagram.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯")
        return
    
    try:
        print(f"ğŸŒ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡: {INSTAGRAM_URL}")
        driver.get(INSTAGRAM_URL)
        time.sleep(5)  # Ù…Ù†ØªØ¸Ø± Ù„ÙˆØ¯ Ø´Ø¯Ù† ØµÙØ­Ù‡
        
        # Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù„Ø§Ú¯ÛŒÙ† Ø¨Ø§Ø´Ø¯
        try:
            login_prompt = driver.find_element(By.XPATH, "//button[contains(text(), 'Log in')]")
            if login_prompt:
                print("âš ï¸  Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù„Ø§Ú¯ÛŒÙ† Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø¯Ø³ØªÛŒ Ù„Ø§Ú¯ÛŒÙ† Ú©Ù†ÛŒØ¯ Ùˆ Enter Ø¨Ø²Ù†ÛŒØ¯...")
                input("Ù¾Ø³ Ø§Ø² Ù„Ø§Ú¯ÛŒÙ†ØŒ Enter Ø±Ø§ Ø¨Ø²Ù†ÛŒØ¯...")
        except:
            pass
        
        # Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡
        scroll_page(driver)
        time.sleep(2)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ URLÙ‡Ø§
        urls = extract_media_urls(driver)
        
        # Ø°Ø®ÛŒØ±Ù‡ URLÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„
        urls_file = os.path.join(os.path.dirname(DOWNLOAD_FOLDER), "instagram_urls.json")
        with open(urls_file, 'w', encoding='utf-8') as f:
            json.dump(urls, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ URLÙ‡Ø§ Ø¯Ø± {urls_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯\n")
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
        if urls['images']:
            print("ğŸ“¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±...")
            for i, url in enumerate(urls['images'], 1):
                print(f"[{i}/{len(urls['images'])}] ", end='')
                download_file(url, DOWNLOAD_FOLDER)
                time.sleep(0.5)
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§
        if urls['videos']:
            print("\nğŸ¬ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§...")
            for i, url in enumerate(urls['videos'], 1):
                print(f"[{i}/{len(urls['videos'])}] ", end='')
                download_file(url, DOWNLOAD_FOLDER)
                time.sleep(1)
        
        print(f"\nâœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"ğŸ“ Ù…Ø­Ù„ Ø°Ø®ÛŒØ±Ù‡: {DOWNLOAD_FOLDER}")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
    finally:
        print("\nğŸ”’ Ø¨Ø³ØªÙ† Ù…Ø±ÙˆØ±Ú¯Ø±...")
        driver.quit()

if __name__ == "__main__":
    main()

